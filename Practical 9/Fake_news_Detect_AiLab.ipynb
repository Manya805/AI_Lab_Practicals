{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k\n",
        "!pip install lxml_html_clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TorAB2YZWB4m",
        "outputId": "e18cf45d-1753-42ed-cfbe-8df342520c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.3.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.1)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.2.0)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2025.1.31)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from lxml_html_clean) (5.3.1)\n",
            "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import pipeline\n",
        "import urllib.request\n",
        "\n",
        "# -------- Train TF-IDF + Logistic Regression --------\n",
        "def train_model():\n",
        "    fake = pd.read_csv(\"Fake.csv\")\n",
        "    true = pd.read_csv(\"True.csv\")\n",
        "\n",
        "    fake[\"label\"], true[\"label\"] = 0, 1\n",
        "    df = pd.concat([fake, true])[[\"text\", \"label\"]].dropna()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
        "    X = vectorizer.fit_transform(df[\"text\"])\n",
        "    y = df[\"label\"]\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    return vectorizer, model\n",
        "\n",
        "vectorizer, tfidf_model = train_model()\n",
        "\n",
        "# -------- Transformers Zero-Shot Classifier --------\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# -------- Google Fact Check --------\n",
        "API_KEY = \"AIzaSyCrJzQ3Io7ji7xW_ermckT20XByvTlb63k\"\n",
        "\n",
        "def google_fact_check(query):\n",
        "    url = f\"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={API_KEY}\"\n",
        "    res = requests.get(url)\n",
        "    if res.status_code == 200:\n",
        "        claims = res.json().get(\"claims\", [])\n",
        "        for c in claims:\n",
        "            for review in c.get(\"claimReview\", []):\n",
        "                rating = review.get(\"textualRating\", \"\").lower()\n",
        "                if \"fake\" in rating or \"false\" in rating:\n",
        "                    return \"âŒ Verified Fake by Google Fact Check\"\n",
        "                elif \"true\" in rating:\n",
        "                    return \"âœ… Verified True by Google Fact Check\"\n",
        "    return None\n",
        "\n",
        "# -------- Article Scraper --------\n",
        "def scrape_article(url):\n",
        "    try:\n",
        "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "        req = urllib.request.Request(url, headers=headers)\n",
        "        html = urllib.request.urlopen(req).read()\n",
        "\n",
        "        article = Article(url)\n",
        "        article.set_html(html)\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "# -------- Classify Text --------\n",
        "def classify_text(text, threshold=0.75):\n",
        "    result = classifier(text, candidate_labels=[\"True\", \"False\"])\n",
        "    true_score = result[\"scores\"][result[\"labels\"].index(\"True\")]\n",
        "\n",
        "    if true_score >= threshold:\n",
        "        return \"âœ… True News (via Transformer)\"\n",
        "    elif true_score <= (1 - threshold):\n",
        "        return \"âŒ Fake News (via Transformer)\"\n",
        "\n",
        "    # TF-IDF\n",
        "    pred = tfidf_model.predict(vectorizer.transform([text]))[0]\n",
        "    return \"âœ… True News (via TF-IDF)\" if pred == 1 else \"âŒ Fake News (via TF-IDF)\"\n",
        "\n",
        "# -------- Main Pipeline --------\n",
        "def check_news(text):\n",
        "    g_result = google_fact_check(text)\n",
        "    if g_result:\n",
        "        return g_result\n",
        "    return classify_text(text)\n",
        "\n",
        "# -------- Input & Output --------\n",
        "print(\"=\" * 60)\n",
        "print(\"ðŸ“° Fake News Detection Tool\")\n",
        "print(\"You can paste the news\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "user_input = input(\"ðŸ”¹ Enter text or URL: \").strip()\n",
        "\n",
        "if user_input.startswith(\"http\"):\n",
        "    print(\"\\nðŸŒ Extracting article content from URL...\\n\")\n",
        "    user_input = scrape_article(user_input)\n",
        "\n",
        "if user_input:\n",
        "    print(\"\\nðŸ”Ž Running authenticity checks...\\n\")\n",
        "    result = check_news(user_input)\n",
        "    print(\"ðŸ§¾ Final Verdict:\", result)\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    print(\"âš ï¸ No valid input provided. Please try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERbW5XyHWPpl",
        "outputId": "4ce63abb-57ef-48a1-c5b7-8733202d9ea1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸ“° Fake News Detection Tool\n",
            "You can paste the news\n",
            "============================================================\n",
            "ðŸ”¹ Enter text or URL: https://www.hindustantimes.com/india-news/justice-br-gavai-to-be-the-next-chief-justice-of-india-101744794329277.html\n",
            "\n",
            "ðŸŒ Extracting article content from URL...\n",
            "\n",
            "\n",
            "ðŸ”Ž Running authenticity checks...\n",
            "\n",
            "ðŸ§¾ Final Verdict: âœ… True News (via Transformer)\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}